%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University Assignment Title Page 
% LaTeX Template
% Version 1.0 (27/12/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% WikiBooks (http://en.wikibooks.org/wiki/LaTeX/Title_Creation)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
% Instructions for using this template:
% This title page is capable of being compiled as is. This is not useful for 
% including it in another document. To do this, you have two options: 
%
% 1) Copy/paste everything between \begin{document} and \end{document} 
% starting at \begin{titlepage} and paste this into another LaTeX file where you 
% want your title page.
% OR
% 2) Remove everything outside the \begin{titlepage} and \end{titlepage} and 
% move this file to the same directory as the LaTeX file you wish to add it to. 
% Then add \input{./title_page_1.tex} to your LaTeX file where you want your
% title page.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\title{Title page with logo}
%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}

\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE Universidade Federal de Alagoas}\\[1.5cm] % Name of your university/college
\textsc{\Large Instituto de Computação (IC)}\\[0.5cm] % Major heading such as course name
\textsc{\large Laboratório de Computação Científica e Análise Numérica (LaCCAN)}\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \LARGE \bfseries Relatório sobre Estimação por Máxima Verossimilhança e Estimação por Momentos}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Autor:}\\
Marcos G. S. do Nascimento % Your name
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Orientador:} \\
Alejandro C. Frery  % Supervisor's Name
\end{flushright}
\end{minipage}\\[2cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large 18/07/2018}\\[2cm] % Date, change the \today to a set date if you want to be precise

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

% \includegraphics{logo.png}\\[1cm] % Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}


\section{Introdução à Estimação de Parâmetros}

 Uma variável aleatória é caracterizada ou descrita pela sua distribuição de probabilidade. Esta, por sua vez, é descrita pelos seus parâmetros populacionais. Existe um grande interesse em conhecer os parâmetros populacionais da distribuição que se está trabalhando para conhecer como os dados estão se comportando. Como geralmente tais parâmetros não são conhecidos, é preciso desenvolver procedimentos para estimá-los.
 
As estimativas dos parâmetros populacionais da distribuição são realizadas a partir dos resultados (dados) de uma variável aleatória de uma amostra representativa extraída da população em questão. A esse procedimento damos o nome de estatística inferencial, visto que se está inferindo algo da população a partir de uma amostra populacional.

Existem duas formas de estimação de parâmetros: por ponto (pontual) ou por intervalo de confiança. 

De forma breve, a estimativa pontual é um valor obtido a partir dos resultados (dados) de uma variável aleatória de uma amostra representativa da população. Por outro lado, a estimação de parâmetros por intervalo de confiança consiste em gerar um intervalo, centrado na estimativa pontual, no qual se admite que esteja o parâmetro da população.






\section{Tipos de estimadores}

Existem vários tipos de estimadores de parâmetros, dentre eles: Momentos, Log-momentos, Máxima Verossimilhança, Máxima Verossimilhança Iterada, Métodos Robustos e Estimação por Kernel.

Em particular, neste relatório será relatado exemplos de estimadores calculados por dois estimadores listados acima: pelo \textbf{Método dos Momentos e pelo Método da Máxima Verossimilhança}.


\subsection{Estimação por Máxima Verossimilhança}

A ideia básica deste método é bem intuitiva. Parece razoável que uma boa estimativa do parâmetro desconhecido  \begin{math} \theta \end{math}
 seria o valor que maximiza a probabilidade de se obter os dados que são observados. Por conta dessa ideia é que se denomina método da Máxima Verossimilhança. 
 
Na prática, suponha que tenhamos uma amostra aleatória \begin{math} X_{1}, X_{2}, ..., X_{n} \end{math}
para a qual a função de densidade de probabilidade de cada \begin{math} X_{i} \end{math} é \begin{math} f(x_{i}; \theta)\end{math} 

Então, a função de densidade de probabilidade de junção  de \begin{math} X_{1}, X_{2}, ..., X_{n} \end{math}, que nós chamaremos de \begin{math} L(\theta) \end{math} é:

\begin{equation}
L(\theta) = P (X_{1} = x_{1} ... X_{n} = x_{n}) =  f(x_{1}; \theta) . ... .f(x_{n}; \theta) =  \prod_{1}^{n} f(x_{i}; \theta)
\end{equation}

A primeira igualdade é, naturalmente, apenas a definição da função de densidade de probabilidade conjunta. A segunda igualdade vem do fato de que temos uma amostra aleatória, o que implica, por definição, que cada  \begin{math} X_{i} \end{math} seja independente e igualmente distribuído. A última igualdade apenas usa a notação de produtório. Agora, à luz da ideia básica da estimação por máxima verossimilhança, uma maneira razoável de proceder é tratar a " função de verossimilhança "  \begin{math} L (\theta) \end{math} como uma função de \begin{math} \theta \end{math}, e encontrar o valor do parâmetro que a maximiza.

A seguir estão listados alguns exemplos de estimadores de Máxima Verossimilhança (MV).

\subsection{Exemplos de estimadores de MV}

\subsubsection{Distribuição normal}

Vamos estimar a média da população considerando uma amostrad de tamanho \emph{n}. A função de densidade de probabilidade da Normal é dada por:

\begin{equation}
f(x_{i}, \mu, \sigma^{2}) = \frac{1}{\sigma\sqrt{2\pi}}exp\left [ \frac{-(x_{i} - \mu)}{2\sigma^{2}} \right ]^{2}
\end{equation}

Portanto, a função de Verossimilhança é dada por:

\begin{equation}
L(\mu, \sigma) = \sigma^{-n}(2\pi)^{\frac{-n}{2}}exp\left [  \frac{-1}{2\sigma^2}\sum_{i=1}^{n}(x_{i}-\mu)^{2} \right ]
\end{equation}

 Ao maximizar a função de verossimilhança em relação a  \begin{math}
 \mu
 \end{math} temos que o melhor estimador para \begin{math} \mu \end{math} é dado por:
 
\begin{equation}
\hat{\mu} = \frac{1}{n}\sum_{i=1}^{n}X_{i} = \bar{X}
\end{equation}

\subsubsection{Distribuição Uniforme}

Considere uma amostra de tamanho \emph{n} que segue uma distribuição \begin{math} Uniforme(0, \theta) \end{math} onde \begin{math} \theta \end{math} é desconhecido. Vamos encontrar o estimador de Máxima Verossimilhança para esta amostra.

Temos então que \begin{math}f(x) = \frac{1}{\theta} \end{math} se \begin{math} 0\leq x \geq \theta \end{math} ou 0, caso contrário.

A função de Verossimilhança é dada por:

\begin{equation}
L(x_{1}, x_{2}, ..., x_{n}; \theta) = \frac{1}{\theta}  
\end{equation}

Lembrando que o caso acima é válido para \begin{math} x_{1}, x_{2}, ..., x_{n} \leq \theta \end{math}, valendo 0, caso contrário.

Note que \begin{math} \frac{1}{\theta} \end{math} é uma função decrescente. Então para minimizá-la nós escolhendo o menor valor possível para \begin{math} \theta \end{math}. Considerando o tamanho da amostra, nós precisamos ter \begin{math} \theta \geq x_{i} \end{math}.

Considerando o dito acima, temos que o melhor estimador é dado por:

\begin{equation}
	\hat{\theta} = max(X_{1}, X_{2}, ..., X_{n})
\end{equation}

\subsubsection{Distribuição Exponencial}

Considerando a forma como os estimadores acima foram calculados, temos que para a distribuição exponencial o processo é análogo.

A função de Verossimilhança é dada por:

\begin{equation}
L(x_{1}, x_{2}, ..., x_{n}; \theta) = \prod_{i=1}^{n} \theta e^{-\theta x_{i}} = \theta^{n}e^{-\theta}\sum_{k=1}^{n}x_{i}
\end{equation}

Maximizando essa função, temos:

\begin{equation}
\hat{\theta} = \frac{n}{\sum_{k=1}^{n}X_{i}}
\end{equation}

\subsubsection{Distribuição Binomial}

Agora estamos no contexto de uma \textbf{distribuição discreta}, mas em ambos os casos de termos uma Distribuição Contínua ou Discreta, o estimador de Máxima Verossimilhança para o parâmetro desconhecido \begin{math} \theta \end{math} é o valor discreto ou contínuo que maximiza a função de verossimilhança, independente do tipo de distribuição.

Então, de forma análoga ao que já foi calculado anteriormente, considere uma amostra de tamanho \emph{n} e que 
Xi $\sim$ \begin{math} Binomial(m, \theta) \end{math}. Temos que a função de verossimilhança é calculada como:

\begin{align}
       L(x_1, x_2, \cdots, x_n; \theta)&= f_{X_1 X_2 \cdots X_n}(x_1, x_2, \cdots, x_n; \theta)\\
       &=\prod_{i=1}^{n} f_{X_i}(x_i; \theta)\\
       &=\prod_{i=1}^{n} {m \choose x_i} \theta^{x_i} (1-\theta)^{m-x_i}\\
       &=\left[\prod_{i=1}^{n} {m \choose x_i} \right] \theta^{\sum_{i=1}^n x_i} (1-\theta)^{mn-\sum_{i=1}^n x_i}.
\end{align}

Note que na equação 12 o primeiro termo não depende do parâmetro \begin{math}
\theta \end{math}, então podemos reescrever a equação da seguinte forma:

\begin{equation}
       L(x_1, x_2, \cdots, x_n; \theta)= c \qquad \theta^{s} (1-\theta)^{mn-s},
\end{equation}

onde \emph{c} não depende de \begin{math} \theta \end{math} e \begin{math} s =
\sum_{k=1}^n x_i.
\end{math}

Então, derivando esta equação e igualando a derivada a 0 para encontrar o ponto de máximo, temos que:

\begin{align}
       \hat{\theta}= \frac{1}{mn}\sum_{k=1}^n x_i.
\end{align}

\subsubsection{Distribuição Poisson}

Novamente, suponha que \begin{math} X = (X_1, X_2, ..., X_n) \end{math} são observações iid que segue uma distribuição Poisson com o parâmetro \begin{math} \lambda \end{math} desconhecido. A função de verossimilhança é dada a seguir:

\begin{align}
L(x_1, x_2, \cdots, x_n; \lambda) &= \prod\limits_{i=1}^n f(x_i;\lambda) \\ &= \prod\limits_{i=1}^n \dfrac{\lambda^{x_i}e^{-\lambda}}{x_i!}\\ &= \dfrac{\lambda^{\sum\limits^n_{i=1}x_i} e^{-n\lambda}}{x_1!x_2! \cdots x_n!}\\ 
\end{align}

Diferenciando o log dessa função em relação a \begin{math} \lambda \end{math}, isto é, diferenciando a função log-Verossimilhança de Poisson, temos:

\begin{align}
	l(x_1, x_2, \cdots, x_n; \lambda)=\sum\limits^n_{i=1}x_i \text{ log }\lambda-n\lambda
\end{align}

Ignorando os termos constantes que não dependem do parâmetro em questão, obtemos que o estimador de Máxima Verossimilhança é dado por:

\begin{align}
\hat{\lambda}=\sum\limits^n_{i=1}\frac{x_i}{n}
\end{align}

\subsection{Estimação pelo Método dos Momentos}

Uma outra forma de encontrar estimadores de parâmetros populacionais, como a média e a variância por exemplo, é através do método dos momentos. Este método é baseado nos momentos teóricos e amostrais das variáveis aleatórias envolvidas. É mais simples de calcular em relação ao método anterior de Máxima Verossimilhança, mas pode gerar, com mais frequência, estimadores menos precisos. 

Em suma, o método dos momentos envolve equacionar momentos da amostra com momentos teóricos. Então, vamos começar lembrando as definições dos momentos teóricos, bem como aprender as definições dos momentos das amostras.

A seguir estão algumas definições:

\begin{enumerate}
	\item $E(X^k)$ é o k-ésimo momento teórico da distribuição (sobre a origem)
    \item $E\left[(X-\mu)^k\right]$ é o k-ésimo momento teórico (sobre a média)
    \item $M_k=\dfrac{1}{n}\sum\limits_{i=1}^n X_i^k$ é o k-ésimo momento amostral 
    \item $M_k^\ast =\dfrac{1}{n}\sum\limits_{i=1}^n (X_i-\bar{X})^k$ é o k-ésimo momento amostral sobre a média
    
\end{enumerate}

A ideia por trás desse método é bastante simples. Vejamos os seguintes passos da primeira forma de se calcular os estimadores por este método:

\begin{enumerate}
  \item Equacione o primeiro momento da amostra sobre a origem $M_1=\dfrac{1}{n}\sum\limits_{i=1}^n X_i=\bar{X}$ ao primeiro momento teórico $E(x)$
  \item Equacione o segundo momento da amostra sobre a origem $M_2=\dfrac{1}{n}\sum\limits_{i=1}^n X_i^2$ ao segundo momento teórico $E(x^2)$ 
  \item Continuar igualando momentos amostrais sobre a origem, $M_k$ , com os correspondentes momentos teóricos  $E(X^k)$, $k = 3, 4, ...$, até que se tenha tantas equações quanto se tem de parâmetros.
  \item Por fim, resolva os parâmetros.
  
\end{enumerate}

Os valores resultantes são chamados de estimadores de método de momentos . Parece razoável que esse método forneça boas estimativas, já que a distribuição empírica converge, em certo sentido, para a distribuição de probabilidade. Portanto, os momentos correspondentes devem ser aproximadamente iguais.

Em alguns casos, em vez de usar os momentos da amostra sobre a origem, é mais fácil usar os momentos da amostra sobre a média. Ao fazê-lo, fornece-nos uma forma alternativa do método dos momentos que é análogo ao método anterior só que com a diferença de a partir do segundo momento as equações envolvem os momentos amostrais sobre a média ao invés da origem.



\subsubsection{Distribuição de Bernoulli}

suponha que \begin{math} X = (X_1, X_2, ..., X_n) \end{math} são observações aleatórias iid que seguem uma distribuição Bernoulli com o parâmetro \emph{p} desconhecido. Vamos calcular o estimador de momentos para \emph{p}.

O primeiro momento teórico sobre a origem é:

\begin{equation}
	E(x_i) = p
\end{equation}

Temos apenas um parâmetro para o qual estamos tentando derivar o método do estimador de momentos. Portanto, precisamos apenas de uma equação. Igualando o primeiro momento teórico sobre a origem com o momento de amostra correspondente, obtemos:

\begin{equation}
	\hat{p}=\dfrac{1}{n}\sum\limits_{i=1}^n X_i
\end{equation}

\subsubsection{Distribuição Normal}

suponha que \begin{math} X = (X_1, X_2, ..., X_n) \end{math} são observações aleatórias iid que seguem uma distribuição Bernoulli com os parâmetros média \begin{math} \mu \end{math} e variância \begin{math} \sigma^2 \end{math}. Vamos calcular o estimador de momentos para ambos.

Os primeiros e segundos momentos teóricos sobre a origem são:

\begin{equation}
	E(X_i) = \mu   
\end{equation}

\begin{equation}
	E(X_i^2) = \sigma^2 + \mu^2
\end{equation}

Incidentalmente, esse segundo momento pode ser derivado da manipulação da fórmula de atalho para a variância. Nesse caso, temos dois parâmetros para os quais estamos tentando derivar o método de estimadores de momentos. Portanto, precisamos de duas equações aqui. Igualando o primeiro momento teórico sobre a origem com o momento de amostra correspondente, obtemos:

\begin{equation}
	E(X)=\mu=\dfrac{1}{n}\sum\limits_{i=1}^n X_i
\end{equation}

E, igualando o segundo momento teórico sobre a origem com o momento da amostra correspondente, obtemos:

\begin{equation}
	E(X^2)=\sigma^2+\mu^2=\dfrac{1}{n}\sum\limits_{i=1}^n X_i^2
\end{equation}

Agora, a primeira equação nos diz que o estimador de momentos para a média \begin{math} \mu \end{math} é a média da amostra:

\begin{equation}
	\hat{\mu} = \dfrac{1}{n}\sum\limits_{i=1}^n X_i=\bar{X}
\end{equation}

E, substituindo a média da amostra por \begin{math} \mu \end{math}  na segunda equação e resolvendo para \begin{math} \sigma^2 \end{math}, obtemos que o estimador de momentos para a variância é:

\begin{equation}
	\hat{\sigma}^2=\dfrac{1}{n}\sum\limits_{i=1}^n X_i^2-\mu^2=\dfrac{1}{n}\sum\limits_{i=1}^n X_i^2-\bar{X}^2
\end{equation}

que pode ser reescrito como:

\begin{equation}
	\hat{\sigma}^2=\dfrac{1}{n}\sum\limits_{i=1}^n( X_i-\bar{X})^2
\end{equation}

Para este exemplo, os estimadores de método de momentos são os mesmos dos estimadores de máxima verossimilhança.

\subsubsection{Distribuição Gama}

Suponha que \begin{math} X = (X_1, X_2, ..., X_n) \end{math} são observações aleatórias iid que seguem uma distribuição Gama com os parâmetros \begin{math} \alpha \end{math} e \begin{math} \theta \end{math}.

Utilizando o método da Máxima Verossimilhança é complicado diferenciar por conta da função gama maior dada por \begin{math} \Gamma(\alpha) \end{math}. Então, ao invés de encontrar os estimadores de máxima verossimilhança, vamos encontrar o estimador de momentos para ambos os parâmetros mencionados.

O primeiro momento teórico sobre a origem é: 

\begin{equation}
	E(X_i) = \alpha\theta
\end{equation}

O segundo momento teórico sobre a média é: 

\begin{equation}
	Var(X_i) = E(X_i - \mu) = \alpha\theta^2
\end{equation}

Como temos dois parâmetros para os quais estamos tentando derivar estimadores de momentos, precisamos de duas equações. Igualando o primeiro momento teórico sobre a origem com o momento de amostra correspondente, obtemos:

\begin{equation}
	E(X)=\alpha\theta=\dfrac{1}{n}\sum\limits_{i=1}^n X_i=\bar{X}
\end{equation}

E, igualando o segundo momento teórico sobre a média com o momento da amostra correspondente, obtemos:

\begin{equation}
	Var(X)=\alpha\theta^2=\dfrac{1}{n}\sum\limits_{i=1}^n (X_i-\bar{X})^2
\end{equation}

Agora, temos apenas que resolver os dois parâmetros. Vamos  começar resolvendo \begin{math} \alpha \end{math} na primeira equação (E(X)). Fazendo isso, obtemos:


\begin{equation}
	\alpha=\dfrac{\bar{X}}{\theta}
\end{equation}

Agora, substituindo o valor \begin{math} \alpha \end{math} na segunda equação (Var(x)), temos:

\begin{equation}
\alpha\theta^2=\left(\dfrac{\bar{X}}{\theta}\right)\theta^2=\bar{X}\theta=\dfrac{1}{n}\sum\limits_{i=1}^n (X_i-\bar{X})^2
\end{equation}

Resolvendo para \begin{math} \theta \end{math} nessa última equação, obtemos que o método do estimador de momento é dado por: 

\begin{equation}
\hat{\theta}=\dfrac{1}{n\bar{X}}\sum\limits_{i=1}^n (X_i-\bar{X})^2
\end{equation}

Substituindo de volta esse valor de  \begin{math} \theta \end{math} na equação que temos para \begin{math} \alpha \end{math}, obtemos que o método do estimador de momento para esse parâmetro é dado por:

\begin{equation}
\hat{\alpha}=\dfrac{\bar{X}}{\hat{\theta}_{MM}}=\dfrac{\bar{X}}{(1/n\bar{X})\sum\limits_{i=1}^n (X_i-\bar{X})^2}=\dfrac{n\bar{X}^2}{\sum\limits_{i=1}^n (X_i-\bar{X})^2}
\end{equation}











\end{document}